---
title: "R Notebook"
output: html_notebook
---

## Chapter 3 Visualization

###Real insight?

You are given a stack of 10 employee surveys and told to figure out the team's sentiment. The two question survey has 1 question with a numeric scale (1-10) where employees answer how inspired they are at work and a second question for free form text.

You are asked to perform a sentiment analysis on the free form text. Would performing sentiment analysis on the text be appropriate?

ANSWER THE QUESTION
50XP
Possible Answers
Yes, the sentiment analysis confirms the employee ratings.
press 1
--> correct: No, the free form text will correlate with the ratings and with only 10 surveys the results may have selection and simultaneity bias.

### Unhappy ending? Chronological polarity

Sometimes you want to track sentiment over time. For example, during an ad campaign you could track brand sentiment to see the campaign's effect. You saw a few examples of this at the end of the last chapter.

In this exercise you'll recap the workflow for exploring sentiment over time using the novel Moby Dick. One should expect that happy moments in the book would have more positive words than negative. Conversely dark moments and sad endings should use more negative language. You'll also see some tricks to make your sentiment time series more visually appealling.

Recall that the workflow is:

Inner join the text to the lexicon by word.
Count the sentiments by line.
Reshape the data so each sentiment has its own column.
(Depending upon the lexicon) Calculate the polarity as positive score minus negative score.
Draw the polarity time series.
INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
This exercise should look familiar: it extends Bing tidy polarity: Call me Ishmael (with ggplot2)!.

inner_join() the pre-loaded tidy version of Moby Dick, moby, to the bing lexicon.
Join by the "term" column in the text and the "word" column in the lexicon.
Count by sentiment and index.
Reshape so that each sentiment has its own column using spread() with the column sentiment and the counts column called n.
Also specify fill = 0 to fill out missing values with a zero.
Using mutate() add two columns: polarity and line_number.
Set polarity equal to the positive score minus the negative score.
Set line_number equal to the row number using the row_number() function.
Create a sentiment time series with ggplot().
Pass in moby_polarity to the data argument.
Call aes() and pass in line_number and polarity without quotes.
Add a smoothed curve with geom_smooth().
Add a red horizontal line at zero by calling geom_hline(), with parameters 0 and "red".
Add a title with ggtitle() set to "Moby Dick Chronological Polarity".


```{r}
moby_polarity <- moby %>%
  # Inner join to the lexicon
  inner_join(bing, by = c("term" = "word")) %>%
  # Count by sentiment, index
  count(sentiment, index) %>%
  # Spread sentiments
  spread(sentiment, n, fill = 0) %>%
  mutate(
    # Add polarity field
    polarity = positive - negative,
    # Add line number field
    line_number = row_number()
  )

# Plot
ggplot(moby_polarity, aes(line_number, polarity)) + 
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("Moby Dick Chronological Polarity") +
  theme_gdocs()
```

[resulted plot]("download (3).png")

### Word impact, frequency analysis

One of the easiest ways to explore data is with a frequency analysis. Although not difficult, in sentiment analysis this simple method can be surprisingly illuminating. Specifically, you will build a barplot. In this exercise you are once again working with moby and bing to construct your visual.

INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
Create moby_tidy_sentiment.
Use count() on moby_sents with term, sentiment, and wt = count.
Pipe to spread() with sentiment, n, and fill = 0.
Pipe to mutate(). Call the new variable polarity; calculated as positive - negative.
Call moby_tidy_sentiment to review and compare it to the previous exercise.
Create moby_tidy_small with filter() applied to moby_tidy_sentiment. The logical check should be abs(polarity) >= 50. Using the abs() function returns any rows whose absolute polarity is greater than or equal to 50.
mutate() a new vector pol with an ifelse() function checking if polarity > 0 then declare the document "positive" else declare it "negative".
Using ggplot() pass in moby_tidy_pol, aes(reorder(term, polarity), polarity, fill = pol) which is the data, an X axis reordering terms by polarity, a Y axis represented by the polarity and a fill color defined by the pol designation.
Add geom_bar() with stat = "identity".
In the last layer adjust the theme to theme(axis.text.x = element_text(angle = 90, vjust = -0.1)).

```{r}
# Inner join without renamed columns
moby_sents <- inner_join(moby, bing, by = c("term" = "word"))

# Tidy sentiment calculation
moby_tidy_sentiment <- moby_sents %>% 
  count(term, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(polarity = positive - negative)

# Review
moby_tidy_sentiment

# Subset
moby_tidy_small <- moby_tidy_sentiment %>% 
  filter(abs(polarity) >= 50)

# Add polarity
moby_tidy_pol <- moby_tidy_small %>% 
  mutate(
    pol = ifelse(polarity > 0, "positive", "negative")
  )

# Plot
ggplot(
  moby_tidy_pol, 
  aes(reorder(term, polarity), polarity, fill = pol)
) +
  geom_bar(stat = "identity") + 
  ggtitle("Moby Dick: Sentiment Word Frequency") + 
  theme_gdocs() +
  theme(axis.text.x = element_text(angle = 90, vjust = -0.1))
```

[resulted plot]("download (4).png")

### Divide & conquer: Using polarity for a comparison cloud

Now that you have seen how polarity can be used to divide a corpus, let's do it! This code will walk you through dividing a corpus based on sentiment so you can peer into the informaton in subsets instead of holistically.

Your R session has oz_pol which was created by applying polarity() to "The Wonderful Wizard of Oz."

For simplicity's sake, we created a simple custom function called pol_subsections() which will divide the corpus by polarity score. First, the function accepts a data frame with each row being a sentence or document of the corpus. The data frame is subset anywhere the polarity values are greater than or less than 0. Finally, the positive and negative sentences, non-zero polarities, are pasted with parameter collapse so that the terms are grouped into a single corpus. Lastly, the two documents are concatenated into a single vector of two distinct documents.

pol_subsections <- function(df) {
  x.pos <- subset(df$text, df$polarity > 0)
  x.neg <- subset(df$text, df$polarity < 0)
  x.pos <- paste(x.pos, collapse = " ")
  x.neg <- paste(x.neg, collapse = " ")
  all.terms <- c(x.pos, x.neg)
  return(all.terms)
}
At this point you have omitted the neutral sentences and want to focus on organizing the remaining text. In this exercise we use the %>% operator again to forward objects to functions. After some simple cleaning use comparison.cloud() to make the visual.

INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
Within select() declare the first column text as text.var which is the raw text. The second column polarity should refer to the polarity scores polarity. Piped column names don't need $, you just use the column name without quotes.
Now apply pol_subsections() to oz_df. Call the new object all_terms.
To create all_corpus apply VectorSource() to all_terms and then %>% to VCorpus().
Create a TDM, all_tdm, using TermDocumentMatrix() on all_corpus.
Add in the parameters control = list(removePunctuation = TRUE, stopwords = stopwords(kind = "en"))).
Then %>% to as.matrix() and %>% again to set_colnames(c("positive", "negative")).
Apply comparison.cloud() to all_tdm with parameters max.words = 50, and colors = c("darkgreen","darkred").

```{r}
# Add scores to each document line in a data frame
oz_df <- oz_pol$all %>%
  select(text = text.var, polarity = polarity)

# Custom function
all_terms <- pol_subsections(oz_df)

# Make a corpus
all_corpus <- all_terms %>%
  VectorSource() %>% 
  VCorpus()

# Basic TDM
all_tdm <- TermDocumentMatrix(
  all_corpus,
  control = list(
    removePunctuation = TRUE,
    stopwords = stopwords(kind = "en")
  )
) %>%
  as.matrix() %>%
  set_colnames(c("positive", "negative"))

# Make a comparison cloud
comparison.cloud(
  all_tdm,
  max.words = 50,
  colors = c("darkgreen", "darkred")
)
```
[resulted wordcloud]("download (5).png")

### Emotional introspection

In this exercise you go beyond subsetting on positive and negative language. Instead you will subset text by each of the 8 emotions in Plutchik's emotional wheel to construct a visual. With this approach you will get more clarity in word usage by mapping to a specific emotion instead of just positive or negative.

Using the tidytext subjectivity lexicon, "nrc", you perform an inner_join() with your text. The "nrc" lexicon has the 8 emotions plus positive and negative term classes. So you will have to drop positive and negative words after performing your inner_join(). One way to do so is with the negation, !, and grepl().

The "Global Regular Expression Print Logical," grepl(), function will return a True or False if a string pattern is identified in each row. In this exercise you will search for positive OR negative using the | operator, representing "or" as shown below. Often this straight line is above the enter key on a keyboard. Since the ! negation precedes grepl(), the T or F is switched so the "positive|negative" is dropped instead of kept.

Object <- tibble %>%
  filter(!grepl("positive|negative", column_name))
Next you apply count() on the identified words along with spread() to get the data frame organized.

This exercise introduces rownames(). This function declares the names of rows in a data frame. It behaves a bit differently because rownames() is passed the object gaining the row names on the left side of <-. On the right side the character vector of names is declared such as data_frame[, 1]. For instance:

rownames(data_frame) <- vector_of_names
After setting row names you will create a more varied comparison.cloud().

INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
inner_join() moby and nrc to create moby_sentiment.
Using filter() with a negation (!) and grepl() search for "positive|negative". The column to search is called sentiment.
Forward moby_pos_neg to count()with sentiment then term = term. %>% this to spread() passing in sentiment, n, and fill = 0 and then change it to a data frame with as.data.frame().
Set the rownames() of moby_tidy to be the first column vector, moby_tidy[, 1].
Drop the first column of moby_tidy (since it was declared as row names) by setting moby_tidy[, 1] to NULL.
Examine moby_tidy using head().
Apply comparison.cloud() to moby_tidy with max.words = 50, title.size = 1.5.

```{r}
# Inner join
moby_sentiment <- inner_join(moby, nrc)

# Drop positive or negative
moby_pos_neg <- moby_sentiment %>%
  filter(!grepl("positive|negative", sentiment))

# Count terms by sentiment then spread 
moby_tidy <- moby_pos_neg %>% 
  count(sentiment, term = term) %>% 
  spread(sentiment, n, fill = 0) %>%
  as.data.frame()

# Set row names
rownames(moby_tidy) <- moby_tidy[, 1]

# Drop terms column
moby_tidy[, 1] <- NULL

# Examine
head(moby_tidy)

# Comparison cloud
comparison.cloud(moby_tidy, max.words = 50, title.size = 1.5)
```
[resulted wordcloud]("download (6).png")


### Compare & contrast stacked bar chart

Another way to slice your text is to understand how much of the document(s) are made of positive or negative words. For example a restaurant review may have some positive aspects such as "the food was good" but then continue to add "the restaurant was dirty, the staff was rude and parking was awful." As a result, you may want to understand how much of a document is dedicated to positive vs negative language. In this example it would have a higher negative percentage compared to positive.

One method for doing so is to count() the positive and negative words then divide by the number of subjectivity words identified. In the restaurant review example, "good" would count as 1 positive and "dirty," "rude," and "awful" count as 3 negative terms. A simple calculation would lead you to believe the restaurant review is 25% positive and 75% negative since there were 4 subjectivity terms.

Start by performing the inner_join() on a unified tidy data frame containing 4 books, Agamemnon, Oz, Huck Finn, and Moby Dick. Just like the previous exercise you will use filter() and grepl().

To perform the count() you have to group the data by book and then sentiment. For example all the positive words for Agamemnon have to be grouped then tallied so that positive words from all books are not mixed. Luckily, you can pass multiple variables into count() directly.

INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
Forward book_sents, which is the NRC inner join to all tidy books, to filter().
Perform a grepl() on the sentiment column, checking without the negation so that "positive|negative" are kept.
Review the tail of books_pos_neg.
Create books_sent_count by passing books_pos_neg to count() with parameters book and then sentiment.
Review the entire object by calling books_sent_count.
Create book_pos using group_by() and mutate() a new variable. Pass book as the grouping variable, then calculate the percentage by passing the expression n / sum(n) * 100 inside mutate().
Make a barplot with ggplot() using book_pos with aes(x = book, y = percent_positive, fill = sentiment).
Add another layer with + and pass in geom_bar(stat = "identity").


```{r}
# Review tail of all_books
tail(all_books)

# Inner join
books_sents <- inner_join(all_books, nrc)

# Keep only positive or negative
books_pos_neg <- books_sents %>%
  filter(grepl("positive|negative", sentiment))

# Review tail again
tail(books_pos_neg)

# Count by book & sentiment
books_sent_count <- books_pos_neg %>%
  count(book, sentiment)

# Review entire object
books_sent_count

# Split, make proportional
book_pos <- books_sent_count %>%
  group_by(book) %>% 
  mutate(percent_positive = n / sum(n) * 100)

# Proportional bar plot
ggplot(book_pos, aes(x = book, y = percent_positive, fill = sentiment)) +  
  geom_bar(stat = "identity")
```
[resulted plot]("download (7).png")



### Kernel density plot

Now that you learned about a kernel density plot you can create one! Remember it's like a smoothed histogram but isn't affected by binwidth. This exercise will help you construct a kernel density plot from sentiment values.

In this exercise you will plot 2 kernel densities. One for Agamemnon and another for The Wizard of Oz. For both you will perform an inner_join() with the "afinn" lexicon. Recall the "afinn" lexicon has terms scored from -5 to 5. Once in a tidy format, both books will retain words and corresponding scores for the lexicon.

After that, you need to row bind the results into a larger data frame using rbind() and create a plot with ggplot2.

From the visual you will be able to understand which book uses more positive versus negative language. There is clearly overlap as negative things happen to Dorothy but you could infer the kernel density is demonstrating a greater probability of positive language in the Wizard of Oz compared to Agamemnon.

In this exercise you will declare a new column with a repeating value. Here you are adding a new column by adding it to the data frame as in $new_column_name with a recycled value.

data_frame$new_column_name <- 'repeating value'
INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
We've loaded ag as a tidy version of Agamemnon and created afinn as a subset of the tidytext "afinn" lexicon. Make a new object called ag_afinn using inner_join() on ag and afinn.
Create a new column by declaring ag_afinn$book as "agamemnon". This will append a new column that recycles the string for all rows.
Perform another inner_join() on the preloaded oz and afinn objects. Call the new object oz_afinn.
Append another column called oz_afinn$book with a recycled string "oz".
Declare all_df as a new object using rbind() on ag_afinn and oz_afinn.
Create the kernel density plot with ggplot(). Pass in all_df as your data along with aes(x = score, fill = book). Add the density layer using geom_density() with an alpha = 0.3. Lastly, add a title using ggtitle() called "AFINN Score Densities".

```{r}
# Agamemnon inner join
ag_afinn <- inner_join(ag, afinn)

# Add book
ag_afinn$book <- "agamemnon"

# Oz inner join
oz_afinn <- inner_join(oz, afinn)

# Add book
oz_afinn$book <- "oz"

# Combine
all_df <- rbind(ag_afinn, oz_afinn)

# Plot 2 densities
ggplot(all_df, aes(x = score, fill = book)) + 
  geom_density(alpha = 0.3) + 
  theme_gdocs() +
  ggtitle("AFINN Score Densities")
```

[resulted plot]("download (8).png")


### Box plot

An easy way to compare multiple distributions is with a box plot. This code will help you construct multiple box plots to make a compact visual.

In this exercise the all_book_polarity object is already loaded. The data frame contains two columns, book and polarity. It comprises all books with qdap's polarity() function applied. Here are the first 3 rows of the large object.

book	polarity
14	huck	0.2773501
22	huck	0.2581989
26	huck	-0.5773503
This exercise introduces tapply() which allows you to apply functions over a ragged array. You input a vector of values and then a vector of factors. For each factor, value combination the third parameter, a function like min(), is applied. For example here's some code with tapply() used on two vectors.

f1 <- as.factor(c("Group1", "Group2", "Group1", "Group2"))
stat1 <- c(1, 2, 1, 2)
tapply(stat1, f1, sum)
The result is an array where Group1 has a value of 2 (1+1) and Group2 has a value of 4 (2+2).

INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
Since it's already loaded, examine the all_book_polarity with str().
Using tapply(), pass in all_book_polarity$polarity, all_book_polarity$book and the summary() function. This will print the summary statistics for the 4 books in terms of their polarity() scores. You would expect to see Oz and Huck Finn to have higher averages than Agamemnon or Moby Dick. Pay close attention to the median.
Create a box plot with ggplot() by passing in all_book_polarity.
Aesthetics should be aes(x = book, y = polarity).
Using a + add the geom_boxplot() with col = "darkred". Pay close attention to the dark line in each box representing median.
Next add another layer called geom_jitter() to add points for each of the words.


```{r}
# Examine
str(all_book_polarity)

# Summary by document
tapply(all_book_polarity$polarity, all_book_polarity$book, summary)

# Box plot
ggplot(all_book_polarity, aes(x = book, y = polarity)) +
  geom_boxplot(fill = c("#bada55", "#F00B42", "#F001ED", "#BA6E15"), col = "darkred") +
  geom_jitter(position = position_jitter(width = 0.1, height = 0), alpha = 0.02) +
  theme_gdocs() +
  ggtitle("Book Polarity")
```

[resulted plot]("download (9).png")


### Radar chart

Remember Plutchik's wheel of emotion? The NRC lexicon has the 8 emotions corresponding to the first ring of the wheel. Previously you created a comparison.cloud() according to the 8 primary emotions. Now you will create a radar chart similar to the wheel in this exercise.

A radarchart is a two-dimensional representation of multidimensional data (at least 3). In this case the tally of the different emotions for a book are represented in the chart. Using a radar chart, you can review all 8 emotions simultaneously.

As before we've loaded the "nrc" lexicon as nrc and moby_huck which is a combined tidy version of both Moby Dick and Huck Finn.

In this exercise you once again use a negated grepl() to remove "positive|negative" emotional classes from the chart. As a refresher here is an example:

object <- tibble %>%
  filter(!grepl("positive|negative", column_name))
This exercise reintroduces spread() which rearranges the tallied emotional words. As a refresher consider this raw data called datacamp.

people	food	like
Nicole	bread	78
Nicole	salad	66
Ted	bread	99
Ted	salad	21
If you applied spread() as in spread(datacamp, people, like) the data looks like this.

food	Nicole	Ted
bread	78	99
salad	66	21
INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
Review moby_huck with tail().
inner_join() moby_huck and nrc.
Next, filter() negating "positive|negative" in the sentiment column. Assign the result to books_pos_neg.
After books_pos_neg is forwarded to group_by() with book and sentiment. Then tally() the object with an empty function.
Then spread() the books_tally by the book and n column.
Review the scores data.
Call chartJSRadar() on scores which is an htmlwidget from the radarchart package.

```{r}
# Review tail of moby_huck
tail(moby_huck)

# Inner join
books_sents <- inner_join(moby_huck, nrc)

# Drop positive or negative
books_pos_neg <- books_sents %>%
  filter(!grepl("positive|negative", sentiment))

# Tidy tally
books_tally <- books_pos_neg %>%
  group_by(book, sentiment) %>%
  tally()

# Key value pairs
scores <- books_tally %>%
  spread(book, n)

# Review scores
scores

# JavaScript radar chart
chartJSRadar(scores)
```

[resulted plot]("download (10).png")

### Treemaps for groups of documents

Often you will find yourself working with documents in groups, such as author, product or by company. This exercise lets you learn about the text while retaining the groups in a compact visual. For example, with customer reviews grouped by product you may want to explore multiple dimensions of the customer reviews at the same time. First you could calculate the polarity() of the reviews. Another dimension may be length. Document length can demonstrate the emotional intensity. If a customer leaves a short "great shoes!" one could infer they are actually less enthusiastic compared to a lengthier positive review. You may also want to group reviews by product type such as women's, men's and children's shoes. A treemap lets you examine all of these dimensions.

For text analysis, within a treemap each individual box represents a document such as a tweet. Documents are grouped in some manner such as author. The size of each box is determined by a numeric value such as number of words or letters. The individual colors are determined by a sentiment score.

After you organize the tibble, you use the treemap library containing the function treemap() to make the visual. The code example below declares the data, grouping variables, size, color and other aesthetics.

treemap(data_frame,
        index = c("group", "individual_document"),
        vSize = "V1",
        vColor = "avg_score",
        type = "value",
        title = "Book Sentiment Scores",
        palette = c("red", "white", "green"))
The pre-loaded all_books object contains a combined tidy format corpus with 4 Shakespeare, 3 Melville and 4 Twain books. Based on the treemap you should be able to tell who writes longer books, and the polarity of the author as a whole and for individual books.

INSTRUCTIONS
100XP
INSTRUCTIONS
100XP
inner_join() all_books and the afinnlexicon. Declare the by parameter equal to = c("term" = "word").
Calculate each book's length in a new object called book_length using count() with the book column.
Create book_score with group_by() using author and book then summarize() with a new column mean_score using the mean() function on the score column.
Forward book_score to another inner_join(). The second table is book_length and declare by = "book".
Examine book_tree.
Call treemap() on book_tree followed by index specifying c() with "author" and "book". Declare vSize as "n" and vColor as "mean_score". The type needs to be "value" for appropriate coloring. Finally specify the palette as 

```{r}
books_score <- all_books %>% 
  # Inner join with AFINN scores
  inner_join(afinn, by = c("term" = "word"))

book_length <- books_score %>% 
  # Count number of words per book
  count(book) 

book_score <- books_score %>% 
  # Group by author, book
  group_by(author, book) %>%
  # Calculate mean book score
  summarize(mean_score = mean(score)) 

book_tree <- book_score %>% 
  # Inner join by book
  inner_join(book_length, by = "book")

# Examine the results
book_tree

# Make the visual
treemap(book_tree,
        index = c("author", "book"),
        vSize = "n",
        vColor = "mean_score",
        type = "value",
        title = "Book Sentiment Scores",
        palette = c("red", "white", "green"))
```

[resulted plot]("download (11).png")








