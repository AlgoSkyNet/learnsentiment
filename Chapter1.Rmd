---
title: "R Notebook"
output: html_notebook
---

## Chapter 1 Course

In the first chapter, you will learn how to apply qdap's sentiment function called polarity() .

### Exercise 1 Polarity scoring

```{r}
library(magrittr)
library(qdap)
person <- c("Nick", "Jonathan", "Martijn","Nicole","Nick","Jonathan","Martijn","Nicole")
text <- c("DataCamp courses are the best","I like talking to students","Other online data science curricula are boring.",
          "What is for lunch?", "DataCamp has lots of great content!", "Students are passionate and are excited to learn",
          "Other data science curriculum is hard to learn and difficult to understand",
          "I think the food here is good.")
text_df <- data.frame(person, text)

# Examine the text data
text_df

# Calc overall polarity score
text_df %$% polarity(text)

# Calc polarity score by person
datacamp_conversation <- text_df %$% polarity(text, person)

# Counts table from datacamp_conversation
counts(datacamp_conversation)

# Plot the conversation polarity
plot(datacamp_conversation)
```
### Tm refresher

```{r}
library(tm)
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee"))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}


tm_define <- c("Text mining is the process of distilling actionable insights from text.", 
               "Sentiment analysis represents the set of tools to extract an author's feelings towards a subject.")

# clean_corpus(), tm_define are pre-defined
clean_corpus
tm_define

# Create a VectorSource
tm_vector <- VectorSource(tm_define)

# Apply VCorpus
tm_corpus <- VCorpus(tm_vector)

# Examine the first document's contents
content(tm_corpus[[1]])

# Clean the text
tm_clean <- clean_corpus(tm_corpus)

# Reexamine the contents of the first doc
content(tm_clean[[1]])
```

### Another refresher

```{r}
# clean_text is pre-defined it's a corpora from 1000 tweets about coffee *from bag of words course
clean_text

# Create tf_dtm
tf_dtm <- DocumentTermMatrix(clean_text)

# Create tf_dtm_m
tf_dtm_m <- as.matrix(tf_dtm)

# Dimensions of DTM matrix
dim(tf_dtm_m)

# Subset part of tf_dtm_m for comparison
tf_dtm_m[16:20,2975:2985]
```

### Zipf's Law

```{r}
#install.packages("metricsgraphics")
library(metricsgraphics)

# Examine sb_words
head(sb_words)

# Create expectations
sb_words$expectations <- sb_words %$% 
  {freq / rank}

# Create metrics plot
sb_plot <- mjs_plot(sb_words, x = rank, y = freq, show_rollover_text = FALSE)

# Add 1st line
sb_plot <- mjs_line(sb_plot)

# Add 2nd line
sb_plot <- mjs_add_line(sb_plot, expectations)

# Add legend
sb_plot <- mjs_add_legend(sb_plot, legend = c("Frequency", "Expectation"))

# Display plot
sb_plot

```
### Polarity on actual text

```{r}
# Example statements
positive <- "DataCamp courses are good for learning"

# Calculate polarity of both statements
(pos_score <-polarity(positive))

# Get counts
(pos_counts <- counts(pos_score))
  
# Number of positive words
n_good <- length(pos_counts$pos.words[[1]])
  
# Total number of words
n_words <- pos_counts$wc
  
# Verify polarity score
n_good / sqrt(n_words)
```


### Happy Songs!

```{r}
# Examine conversation
conversation
```
> conversation
  student                                                    text
1 Martijn                            This restaurant is never bad
2    Nick                                 The lunch was very good
3  Nicole It was awful I got food poisoning and was extremely ill

```{r}
# Polarity - All
polarity(conversation$text)
```

> polarity(conversation$text)
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
1 all               3          21        0.317       0.565              0.561

```{r}
# Polarity - Grouped
student_pol <- conversation %$%
  polarity(text, student)

# Student results
scores(student_pol)
```

> scores(student_pol)
  student total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
1 Martijn               1           5        0.447          NA                 NA
2    Nick               1           5        0.805          NA                 NA
3  Nicole               1          11       -0.302          NA                 NA

```{r}
# Sentence by sentence
counts(student_pol)
```

> counts(student_pol)
  student wc polarity pos.words neg.words                                                text.var
1 Martijn  5    0.447         -       bad                            This restaurant is never bad
2    Nick  5    0.805      good         -                                 The lunch was very good
3  Nicole 11   -0.302         -     awful It was awful I got food poisoning and was extremely ill

```{r}
# qdap plot
plot(student_pol)
```


### LOL, this song is wicked good (adjusting lexicons)

 
```{r}
# Examine the key.pol
key.pol
```

> key.pol
               x  y
   1:     a plus  1
   2:   abnormal -1
   3:    abolish -1
   4: abominable -1
   5: abominably -1
  ---              
6775:  zealously -1
6776:     zenith  1
6777:       zest  1
6778:      zippy  1
6779:     zombie -1

```{r}
# Negators
negation.words
```

> negation.words
 [1] "ain't"     "aren't"    "can't"     "couldn't"  "didn't"    "doesn't"  
 [7] "don't"     "hasn't"    "isn't"     "mightn't"  "mustn't"   "neither"  
[13] "never"     "no"        "nobody"    "nor"       "not"       "shan't"   
[19] "shouldn't" "wasn't"    "weren't"   "won't"     "wouldn't"


```{r}
# Amplifiers
amplification.words
```

> amplification.words
 [1] "acute"         "acutely"       "certain"       "certainly"    
 [5] "colossal"      "colossally"    "deep"          "deeply"       
 [9] "definite"      "definitely"    "enormous"      "enormously"   
[13] "extreme"       "extremely"     "great"         "greatly"      
[17] "heavily"       "heavy"         "high"          "highly"       
[21] "huge"          "hugely"        "immense"       "immensely"    
[25] "incalculable"  "incalculably"  "massive"       "massively"    
[29] "more"          "particular"    "particularly"  "purpose"      
[33] "purposely"     "quite"         "real"          "really"       
[37] "serious"       "seriously"     "severe"        "severely"     
[41] "significant"   "significantly" "sure"          "surely"       
[45] "true"          "truly"         "vast"          "vastly"       
[49] "very"


```{r}
# De-amplifiers
deamplification.words
```

> deamplification.words
 [1] "barely"       "faintly"      "few"          "hardly"       "little"      
 [6] "only"         "rarely"       "seldom"       "slightly"     "sparesly"    
[11] "sporadically" "very few"     "very little"

```{r}
# Examine
text
```

> # Examine
> text
  speaker
1 beyonce
2   jay_z
                                                                    words
1 I know I dont understand Just how your love can do what no one else can
2                    They cant figure him out they like hey, is he insane


```{r}
# Explicit polarity parameters
polarity(
  text.var       = text$words,
  grouping.var   = text$speaker,
  polarity.frame = key.pol,
  negators       = negation.words,
  amplifiers     = amplification.words,
  deamplifiers   = deamplification.words 
)
```

  speaker total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
1 beyonce               1          16         0.25          NA                 NA
2   jay_z               1          11         0.00          NA                 NA
> 

### Stressed out!

```{r}
stressed_out <- "I wish I found some better sounds no ones ever heard\nI wish I had a better voice that sang some better words\nI wish I found some chords in an order that is new\nI wish I didnt have to rhyme every time I sang\nI was told when I get older all my fears would shrink\nBut now Im insecure and I care what people think\nMy names Blurryface and I care what you think\nMy names Blurryface and I care what you think\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWere stressed out\nSometimes a certain smell will take me back to when I was young\nHow come Im never able to identify where its coming from\nId make a candle out of it if I ever found it\nTry to sell it never sell out of it Id probably only sell one\nItd be to my brother, cause we have the same nose\nSame clothes homegrown a stones throw from a creek we used to roam\nBut it would remind us of when nothing really mattered\nOut of student loans and tree-house homes we all would take the latter\nMy names Blurryface and I care what you think\nMy names Blurryface and I care what you think\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWe used to play pretend, give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face #\nSaying, Wake up you need to make money\nYeah\nWe used to play pretend give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face\nSaying, Wake up, you need to make money\nYeah\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nWish we could turn back time, to the good old days\nWhen our momma sang us to sleep but now were stressed out\nUsed to play pretend, used to play pretend bunny\nWe used to play pretend wake up, you need the money\nUsed to play pretend used to play pretend bunny\nWe used to play pretend, wake up, you need the money\nWe used to play pretend give each other different names\nWe would build a rocket ship and then wed fly it far away\nUsed to dream of outer space but now theyre laughing at our face\nSaying, Wake up, you need to make money\nYeah"
```

```{r}
library(qdap)
polarity(stressed_out)
```

```{r}
library(qdapDictionaries)
# Check the subjectivity lexicon
key.pol[grep("stress", x)]
```



```{r}
# New lexicon
custom_pol <- sentiment_frame(positive.words, c(negative.words, "stressed", "turn back"))

# Compare new score
polarity(stressed_out, polarity.frame = custom_pol)
```

> polarity(stressed_out, polarity.frame = custom_pol)
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
1 all               1         518       -0.826          NA                 NA


### DTM vs tidytext matrix

```{r}
# As matrix
ag_dtm_m <- as.matrix(ag_dtm)

# Examine line 2206 and columns 245:250
ag_dtm_m[2206, 245:250]

# Tidy up the DTM
ag_tidy <- tidy(ag_dtm)

# Examine tidy with a word you saw
ag_tidy[831:835, ]
```


### Examine the sentiments data frame

```{r}
# Subset to AFINN
afinn_lex <- get_sentiments("afinn")

# Count AFINN scores
afinn_lex %>% 
  count(score)
  
# Subset to nrc
nrc_lex <- get_sentiments("nrc")

# Print nrc_lex
nrc_lex

# Make the nrc counts object
nrc_counts <- nrc_lex %>% 
  count(sentiment)
        
# Barplot
ggplot(nrc_counts, aes(x = sentiment, y = n))+
  geom_bar(stat = "identity") +
  theme_gdocs()
```

### simple example on polarity

```{r}
# Qdap polarity
polarity(ag_txt)
```

> polarity(ag_txt)
Warning message: 
  Some rows contain double punctuation.  Suggested use of `sentSplit` function.
  all total.sentences total.words ave.polarity sd.polarity stan.mean.polarity
1 all             100       15155       -2.783          NA                 NA

```{r}
# Get Bing lexicon
bing <- get_sentiments("bing")

# Join text to lexicon
ag_bing_words <- inner_join(ag_tidy, bing, by = c("term" = "word"))

# Examine
ag_bing_words
```

> ag_bing_words
# A tibble: 483 x 4
   document       term count sentiment
      <chr>      <chr> <dbl>     <chr>
 1        1  abundance     1  positive
 2        1    acclaim     1  positive
 3        1       ache     2  negative
 4        1     aching     3  negative
 5        1 affliction     1  negative
 6        1    affront     1  negative
 7        1     afraid     2  negative
 8        1     aghast     1  negative
 9        1      agony     5  negative
10        1      amply     1  positive
# ... with 473 more rows

```{r}


# Get counts by sentiment
ag_bing_words %>%
  count(sentiment)
```

> ag_bing_words %>%
    count(sentiment)
# A tibble: 2 x 2
  sentiment     n
      <chr> <int>
1  negative   321
2  positive   162

